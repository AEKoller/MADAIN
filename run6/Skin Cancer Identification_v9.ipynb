{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Proportional Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to Visualize the model\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Filepaths, pandas, numpy, Tensorflow, and scikit-image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratify Images in Main Image Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file = \"Resources/HAM10000_metadata.csv\"\n",
    "metadata = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the source directory where all the images are located\n",
    "source_dir = \"Resources/Skin Cancer\"\n",
    "\n",
    "# Define the target directories for train and val splits\n",
    "train_dir = \"Resources/Skin Cancer/train\"\n",
    "val_dir = \"Resources/Skin Cancer/val\"\n",
    "\n",
    "# Define the split ratio (e.g., 0.8 for 80% train, 0.2 for 20% val)\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Create the target directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Get the unique class labels\n",
    "class_labels = metadata[\"dx\"].unique()\n",
    "\n",
    "# Create subdirectories for each class in train and val directories\n",
    "for label in class_labels:\n",
    "    os.makedirs(os.path.join(train_dir, label), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_dir, label), exist_ok=True)\n",
    "\n",
    "# Split the metadata into train and validation sets using stratified splitting\n",
    "# Stratify was required here to ensure that an 80-20 split occured for all classes in the dataset, not just an 80-20 split of the entire dataset.\n",
    "train_metadata, val_metadata = train_test_split(\n",
    "    metadata, stratify=metadata[\"dx\"], test_size=1 - split_ratio, random_state=42\n",
    ")\n",
    "\n",
    "# Move or copy the images to the respective train and validation directories\n",
    "for _, row in train_metadata.iterrows():\n",
    "    image_id = row[\"image_id\"]\n",
    "    image_path = os.path.join(source_dir, f\"{image_id}.jpg\")\n",
    "    class_label = row[\"dx\"]\n",
    "    target_dir = os.path.join(train_dir, class_label)\n",
    "    shutil.copy(image_path, target_dir)\n",
    "\n",
    "for _, row in val_metadata.iterrows():\n",
    "    image_id = row[\"image_id\"]\n",
    "    image_path = os.path.join(source_dir, f\"{image_id}.jpg\")\n",
    "    class_label = row[\"dx\"]\n",
    "    target_dir = os.path.join(val_dir, class_label)\n",
    "    shutil.copy(image_path, target_dir)\n",
    "\n",
    "print(\"Stratified splitting and image organization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n",
      "Class Names: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "Train Dataset Shape: (600, 450, 3)\n",
      "Validation Dataset Shape: (600, 450, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to your image directory\n",
    "image_directory = \"Resources/Skin Cancer\"\n",
    "\n",
    "# Defining original image size\n",
    "image_size = (600, 450)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    " \n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,  # Normalize pixel values\n",
    ")\n",
    "\n",
    "# Load and preprocess the train dataset with data augmentation\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    directory=os.path.join(image_directory, \"train\"),  # Use the 'train' directory\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Create an ImageDataGenerator for validation data (no augmentation)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255)  # Normalize pixel values\n",
    "\n",
    "# Load and preprocess the validation dataset without data augmentation\n",
    "val_dataset = val_datagen.flow_from_directory(\n",
    "    directory=os.path.join(image_directory, \"val\"),  # Use the 'val' directory\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Print the class names\n",
    "class_names = list(train_dataset.class_indices.keys())\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# Print the shape of the datasets\n",
    "print(\"Train Dataset Shape:\", train_dataset.image_shape)\n",
    "print(\"Validation Dataset Shape:\", val_dataset.image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Weight Adjustment\n",
    "This section is to adjust the loss function to more heavily weight our cancerous classes - 'mel', 'akeic', and 'bbc'. This will account for the class imbalance in our original data sets to give us more accurate predictions on the cancerous classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CSV file to store the model results\n",
    "csv_file = \"model_results.csv\"\n",
    "fieldnames = [\n",
    "    \"architecture\",\n",
    "    \"optimizer\",\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"auc\",\n",
    "]\n",
    "\n",
    "# Write the header to the CSV file. Data is written to the file after model_result\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Define batch size, architecture, and optimizer\n",
    "batch_size = 32\n",
    "architectures = [\"InceptionV3\"]\n",
    "optimizers = [\"Adam\"]\n",
    "\n",
    "# Create directory to store models\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model_results to store the evaluation results\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 7 classes.\n",
      "Original Class Weights: [ 4.36859324  2.78484532  1.30212904 12.44099379  1.28603531  0.21338021\n",
      " 10.04010025]\n",
      "Original Class Weight Dictionary: {0: 4.368593238822246, 1: 2.7848453249913105, 2: 1.3021290427433772, 3: 12.440993788819876, 4: 1.2860353130016051, 5: 0.21338020666879728, 6: 10.040100250626567}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Initialize an empty dictionary to store the adjusted class weights\n",
    "adjusted_class_weights = {}\n",
    "\n",
    "# Loop through architectures and optimizers\n",
    "for architecture in architectures:\n",
    "    for optimizer_name in optimizers:\n",
    "        # Define the optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            optimizer = RMSprop(learning_rate=0.001)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "        # Set the input shape and preprocessing function based on the selected architecture\n",
    "        if architecture == \"VGG16\":\n",
    "            input_shape = (224, 224, 3)\n",
    "            preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "        elif architecture == \"ResNet50\":\n",
    "            input_shape = (224, 224, 3)\n",
    "            preprocessing_function = tf.keras.applications.resnet50.preprocess_input\n",
    "        elif architecture == \"InceptionV3\":\n",
    "            input_shape = (299, 299, 3)\n",
    "            preprocessing_function = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "        # Load and preprocess data using tf.keras.preprocessing\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing_function,\n",
    "        )\n",
    "\n",
    "        train_dataset = train_datagen.flow_from_directory(\n",
    "            \"Resources/Skin Cancer/train\",\n",
    "            target_size=input_shape[:2],\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "        # Define num_classes based on the number of classes in the dataset\n",
    "        num_classes = len(train_dataset.class_indices)\n",
    "\n",
    "        # Calculate original class weights\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_dataset.classes),\n",
    "            y=train_dataset.classes\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        print(\"Original Class Weights:\", class_weights)\n",
    "        print(\"Original Class Weight Dictionary:\", class_weight_dict)\n",
    "\n",
    "        # Calculate class frequencies\n",
    "        class_frequencies = np.bincount(train_dataset.classes)\n",
    "\n",
    "        # Compute the multipliers for each class as the inverse of its frequency\n",
    "        class_multipliers = 1 / class_frequencies\n",
    "\n",
    "        # Normalize the multipliers\n",
    "        class_multipliers /= np.sum(class_multipliers)\n",
    "\n",
    "        # Create a dictionary to store adjusted class weights\n",
    "        adjusted_class_weights = {i: multiplier for i, multiplier in enumerate(class_multipliers)}\n",
    "\n",
    "        # Define the loss function with the manually adjusted class weights\n",
    "        def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Custom loss function for weighted categorical crossentropy.\n",
    "\n",
    "            Args:\n",
    "                y_true (tensor): True labels.\n",
    "                y_pred (tensor): Predicted probabilities.\n",
    "\n",
    "            Returns:\n",
    "                tensor: Weighted categorical crossentropy loss.\n",
    "            \"\"\"\n",
    "\n",
    "            # Define a tensor containing the class weights\n",
    "            weights = tf.constant([adjusted_class_weights[class_num] for class_num in range(num_classes)], dtype=tf.float32)\n",
    "\n",
    "            # Clip predicted probabilities to avoid numerical instability\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "            # Compute the categorical cross-entropy loss with weighted class probabilities\n",
    "            loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred) * weights, axis=-1))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Define the pre-trained model architecture\n",
    "        if architecture == \"VGG16\":\n",
    "            base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "        elif architecture == \"ResNet50\":\n",
    "            base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "        elif architecture == \"InceptionV3\":\n",
    "            base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "        # Freeze the layers of the pre-trained model\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top of the pre-trained model\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        # Create the final model\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        # Compile the model with the weighted loss function\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=weighted_categorical_crossentropy,\n",
    "            metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Class Weights:\n",
      "0: 0.1346831559349418\n",
      "1: 0.08585641570548601\n",
      "2: 0.04014446741178015\n",
      "3: 0.3835542049451603\n",
      "4: 0.03964829983702781\n",
      "5: 0.006578483753720125\n",
      "6: 0.30953497241188377\n",
      "{0: 0.1346831559349418, 1: 0.08585641570548601, 2: 0.04014446741178015, 3: 0.3835542049451603, 4: 0.03964829983702781, 5: 0.006578483753720125, 6: 0.30953497241188377}\n"
     ]
    }
   ],
   "source": [
    "# Print adjusted class weights\n",
    "print(\"Adjusted Class Weights:\")\n",
    "for class_num, class_weight in adjusted_class_weights.items():\n",
    "    print(f\"{class_num}: {class_weight}\")\n",
    "\n",
    "#Print adjusted class weights dictionary\n",
    "print(adjusted_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-step Model Building\n",
    "This section contains a cell-by-cell break down of the model creation, and allows for the testing of singular models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Architecture\n",
    "architecture = \"InceptionV3\"  # Options: \"VGG16\", \"ResNet50\", \"InceptionV3\"\n",
    "\n",
    "# Set the input shape and preprocessing function based on the selected architecture\n",
    "if architecture == \"VGG16\":\n",
    "    input_shape = (224, 224, 3)\n",
    "    preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "if architecture == \"resnet50\":\n",
    "    input_shape = (224, 224, 3)\n",
    "    preprocessing_function = tf.keras.applications.resnet.preprocess_input\n",
    "elif architecture == \"InceptionV3\":\n",
    "    input_shape = (299, 299, 3)\n",
    "    preprocessing_function = tf.keras.applications.inception_v3.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using tf.keras.preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function,\n",
    "    # rotation_range=20,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # horizontal_flip=True,\n",
    ")\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    \"Resources/Skin Cancer/train\",\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "val_dataset = val_datagen.flow_from_directory(\n",
    "    \"Resources/Skin Cancer/val\",\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pre-trained model architecture\n",
    "if architecture == \"VGG16\":\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "elif architecture == \"ResNet50\":\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "elif architecture == \"InceptionV3\":\n",
    "    base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique classes from the train_dataset\n",
    "num_classes = len(train_dataset.class_indices)\n",
    "\n",
    "# Add custom layers on top of the pre-trained model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "num_classes = len(train_dataset.class_indices)\n",
    "predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an Optimizer\n",
    "optimizer_name = \"Adam\"  # Options: \"Adam\", \"RMSprop\", \"SGD\"\n",
    "\n",
    "if optimizer_name == \"Adam\":\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "elif optimizer_name == \"RMSprop\":\n",
    "    optimizer = RMSprop(learning_rate=0.001)\n",
    "elif optimizer_name == \"SGD\":\n",
    "    optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, verbose=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Callbacks for TensorBoard and CSV tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "tensorboard_callback = TensorBoard(log_dir=f\"./logs/{architecture}_{optimizer_name}\", histogram_freq=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"models/model_{architecture}_{optimizer_name}.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    ")\n",
    "csv_logger = CSVLogger(f\"models/model_{architecture}_{optimizer_name}_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with callbacks\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=train_dataset.samples // 32,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=val_dataset.samples // 32,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tensorboard_callback, lr_scheduler, checkpoint_callback, csv_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set and print the results\n",
    "loss, accuracy, precision, recall, auc = model.evaluate(val_dataset)\n",
    "print(f\"Model: {architecture}, Optimizer: {optimizer_name}\")\n",
    "print(f\"Validation Loss: {loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {precision:.4f}\")\n",
    "print(f\"Validation Recall: {recall:.4f}\")\n",
    "print(f\"Validation AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(f\"models/model_{architecture}_{optimizer_name}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Testing \n",
    "This section contains a conglomerated model run. It was created so that the models could be tested overnight. I'm going to bed now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 14/250\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:05\u001b[0m 1s/step - accuracy: 0.1338 - auc_2: 0.5277 - loss: 0.0034 - precision_2: 0.1248 - recall_2: 0.0462"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_56368\\1286327115.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;31m# Create learning rate scheduler that monitors validation loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         lr_scheduler = ReduceLROnPlateau(\n\u001b[0;32m    178\u001b[0m             \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         )\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Compile the model with the weighted loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         model.compile(\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     callbacks.on_train_batch_end(\n\u001b[0;32m    327\u001b[0m                         \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     )\n\u001b[0;32m    329\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Override with model metrics instead of last step logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\callbacks\\callback_list.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\callbacks\\progbar_logger.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_init_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# One-indexed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\utils\\progbar.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_order\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0minfo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\" - {k}:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                     avg = backend.convert_to_numpy(\n\u001b[1;32m--> 163\u001b[1;33m                         backend.numpy.mean(\n\u001b[0m\u001b[0;32m    164\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                         )\n\u001b[0;32m    166\u001b[0m                     )\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis, keepdims)\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"int\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mori_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bool\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0mresult_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mori_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m     output = tf.reduce_mean(\n\u001b[0m\u001b[0;32m    499\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    501\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2542\u001b[0m   \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2543\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   2544\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2545\u001b[0m       gen_math_ops.mean(\n\u001b[1;32m-> 2546\u001b[1;33m           \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2547\u001b[0m           name=name))\n",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2046\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2047\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m       \u001b[1;31m# Otherwise, we rely on Range and Rank to do the right thing at run-time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2050\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   2017\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   8021\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8023\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8024\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8025\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8026\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8027\u001b[0m       return _range_eager_fallback(\n\u001b[0;32m   8028\u001b[0m           start, limit, delta, name=name, ctx=_ctx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Importing Dependencies\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Run Number - use to create new directory or add to existing directory\n",
    "run_number = 6\n",
    "run_dir = f\"run{run_number}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Creating a CSV file to store the model results\n",
    "csv_file = \"model_results.csv\"\n",
    "fieldnames = [\n",
    "    \"architecture\",\n",
    "    \"optimizer\",\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"auc\",\n",
    "]\n",
    "\n",
    "# Write the header to the CSV file. Data is written to the file after model_result\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "batch_size = 32\n",
    "architectures = [\"InceptionV3\"] # Options: \"VGG16\", \"ResNet50\", \"InceptionV3\"\n",
    "optimizers = [\"Adam\"] # Options: \"Adam\", \"RMSprop\", \"SGD\"\n",
    "\n",
    "# Create directory to store models\n",
    "models_dir = f\"{run_dir}/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model_results to store the evaluation results\n",
    "model_results = []\n",
    "\n",
    "for architecture in architectures:\n",
    "    # Set the input shape and preprocessing function based on the selected architecture\n",
    "    if architecture == \"VGG16\" or architecture == \"ResNet50\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif architecture == \"ResNet50\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        preprocessing_function = tf.keras.applications.resnet50.preprocess_input\n",
    "    elif architecture == \"InceptionV3\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        preprocessing_function = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # Load and preprocess data using tf.keras.preprocessing\n",
    "    # This adds data augmentation to the training dataset\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_function,\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "        \"Resources/Skin Cancer/train\",\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "    # Define the validation dataset without data augmentation\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        \"Resources/Skin Cancer/val\",\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    # Define the pre-trained model architecture. Will select proper model based on current 'architecture' in for loop\n",
    "    if architecture == \"VGG16\":\n",
    "        base_model = VGG16(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "    elif architecture == \"ResNet50\":\n",
    "        base_model = ResNet50(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "    elif architecture == \"InceptionV3\":\n",
    "        base_model = InceptionV3(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "\n",
    "    # Freeze the layers of the pre-trained model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get the number of unique classes from the train_dataset\n",
    "    num_classes = len(train_dataset.class_indices)\n",
    "\n",
    "    # Add custom layers on top of the pre-trained model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Step through and adjust for each optimizer\n",
    "    for optimizer_name in optimizers:\n",
    "        # Define the optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            optimizer = RMSprop(learning_rate=0.001)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "        # Create learning rate scheduler that monitors validation loss\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.1, patience=5, verbose=1\n",
    "        )\n",
    "\n",
    "        # Compile the model with the weighted loss function\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=weighted_categorical_crossentropy,  # Use the custom loss function\n",
    "            metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "        )\n",
    "\n",
    "        # Create a TensorBoard callback with a separate log directory for each model and optimizer\n",
    "        tensorboard_callback = TensorBoard(\n",
    "            log_dir=f\"./{run_dir}/logs/{architecture}_{optimizer_name}\", histogram_freq=1\n",
    "        )\n",
    "\n",
    "        # Create an EarlyStopping callback to prevent overfitting\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Create a ModelCheckpoint callback to save the best model weights\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                models_dir, f\"model_{architecture}_{optimizer_name}.weights.h5\"\n",
    "            ),\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Create a CSVLogger callback with keras\n",
    "        csv_logger = CSVLogger(\n",
    "            os.path.join(\n",
    "                models_dir, f\"model_{architecture}_{optimizer_name}_training.csv\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # IMPORTANT: RESEARACH CLASS WEIGHTING\n",
    "        # Calculate class weights\n",
    "        # This step is required, as some of the classes are imbalanced, in particular, the val classes\n",
    "        # class_weights = class_weight.compute_class_weight(\n",
    "        #     'balanced',\n",
    "        #     classes=np.unique(train_dataset.classes),\n",
    "        #     y=train_dataset.classes\n",
    "        # )\n",
    "        # class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        # Train the model with the checkpoint callback and weights\n",
    "        epochs = 10\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            steps_per_epoch=train_dataset.samples // batch_size,\n",
    "            validation_data=val_dataset,\n",
    "            validation_steps=val_dataset.samples // batch_size,\n",
    "            epochs=epochs,\n",
    "            class_weight=adjusted_class_weights,\n",
    "            callbacks=[tensorboard_callback, lr_scheduler, checkpoint_callback, csv_logger],\n",
    "        )\n",
    "\n",
    "        # Save the optimizer state to avoid retraining the model\n",
    "        model.save(\n",
    "            os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}.h5\")\n",
    "        )\n",
    "\n",
    "        # Evaluate the model on the validation set and print the results\n",
    "        loss, accuracy, precision, recall, auc = model.evaluate(val_dataset)\n",
    "        print(f\"Model: {architecture}, Optimizer: {optimizer_name}\")\n",
    "        print(f\"Validation Loss: {loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"Validation AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "        # Append the model results to the list\n",
    "        model_result = {\n",
    "            \"architecture\": architecture,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"auc\": auc,\n",
    "        }\n",
    "        model_results.append(model_result)\n",
    "\n",
    "        # Write the current model result to the CSV file\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writerow(model_result)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(\n",
    "            os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}.h5\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Class Weights:\n",
      "0: 0.1346831559349418\n",
      "1: 0.08585641570548601\n",
      "2: 0.04014446741178015\n",
      "3: 0.3835542049451603\n",
      "4: 0.03964829983702781\n",
      "5: 0.006578483753720125\n",
      "6: 0.30953497241188377\n",
      "{0: 0.1346831559349418, 1: 0.08585641570548601, 2: 0.04014446741178015, 3: 0.3835542049451603, 4: 0.03964829983702781, 5: 0.006578483753720125, 6: 0.30953497241188377}\n"
     ]
    }
   ],
   "source": [
    "# Print adjusted class weights\n",
    "print(\"Adjusted Class Weights:\")\n",
    "for class_num, class_weight in adjusted_class_weights.items():\n",
    "    print(f\"{class_num}: {class_weight}\")\n",
    "\n",
    "#Print adjusted class weights dictionary\n",
    "print(adjusted_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Weight Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 7 classes.\n",
      "Found 2003 images belonging to 7 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 26s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Importing Dependencies\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Creating a CSV file to store the model results\n",
    "csv_file = \"model_results.csv\"\n",
    "fieldnames = [\n",
    "    \"architecture\",\n",
    "    \"optimizer\",\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"auc\",\n",
    "]\n",
    "\n",
    "# Write the header to the CSV file. Data is written to the file after model_result\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "batch_size = 32\n",
    "architectures = [\"InceptionV3\"]\n",
    "optimizers = [\"Adam\"]\n",
    "\n",
    "# Create directory to store models\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model_results to store the evaluation results\n",
    "model_results = []\n",
    "\n",
    "for architecture in architectures:\n",
    "    # Set the input shape and preprocessing function based on the selected architecture\n",
    "    if architecture == \"VGG16\" or architecture == \"ResNet50\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif architecture == \"InceptionV3\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        preprocessing_function = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # Load and preprocess data using tf.keras.preprocessing\n",
    "    # This is add data augmentation to the training dataset\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_function,\n",
    "        # rotation_range=20,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # horizontal_flip=True,\n",
    "    )\n",
    "\n",
    "    train_dataset = train_datagen.flow_from_directory(\n",
    "        \"Resources/Skin Cancer/train\",\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "    # Define the validation dataset without data augmentation\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        \"Resources/Skin Cancer/val\",\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    # Define the pre-trained model architecture. Will select proper model based on current 'architecture' in for loop\n",
    "    if architecture == \"VGG16\":\n",
    "        base_model = VGG16(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "    elif architecture == \"ResNet50\":\n",
    "        base_model = ResNet50(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "    elif architecture == \"InceptionV3\":\n",
    "        base_model = InceptionV3(\n",
    "            weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "        )\n",
    "\n",
    "    # Freeze the layers of the pre-trained model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get the number of unique classes from the train_dataset\n",
    "    num_classes = len(train_dataset.class_indices)\n",
    "\n",
    "    # Add custom layers on top of the pre-trained model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Step through and adjust for each optimizer\n",
    "    for optimizer_name in optimizers:\n",
    "        # Define the optimizer\n",
    "        # Learning rate is the size of the step the optimizer will take to minimize the loss function\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            optimizer = RMSprop(learning_rate=0.001)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "        # Create learning rate scheduler that monitors validation loss\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.1, patience=5, verbose=1\n",
    "        )\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "        )\n",
    "\n",
    "        # Create a TensorBoard callback with a separate log directory for each model and optimizer\n",
    "        tensorboard_callback = TensorBoard(\n",
    "            log_dir=f\"./logs/{architecture}_{optimizer_name}\", histogram_freq=1\n",
    "        )\n",
    "\n",
    "        # Create an EarlyStopping callback to prevent overfitting\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Create a ModelCheckpoint callback to save the best model weights\n",
    "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(\n",
    "                models_dir, f\"model_{architecture}_{optimizer_name}.weights.h5\"\n",
    "            ),\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        # Create a CSVLogger callback with keras\n",
    "        csv_logger = CSVLogger(\n",
    "            os.path.join(\n",
    "                models_dir, f\"model_{architecture}_{optimizer_name}_training.csv\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate class weights\n",
    "        # This step is required, as some of the classes are imbalanced, in particular, the val classes\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(train_dataset.classes),\n",
    "            y=train_dataset.classes\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.36859324  2.78484532  1.30212904 12.44099379  1.28603531  0.21338021\n",
      " 10.04010025]\n",
      "{0: 4.368593238822246, 1: 2.7848453249913105, 2: 1.3021290427433772, 3: 12.440993788819876, 4: 1.2860353130016051, 5: 0.21338020666879728, 6: 10.040100250626567}\n",
      "akiec: 4.368593238822246\n",
      "bcc: 2.7848453249913105\n",
      "bkl: 1.3021290427433772\n",
      "df: 12.440993788819876\n",
      "mel: 1.2860353130016051\n",
      "nv: 0.21338020666879728\n",
      "vasc: 10.040100250626567\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)\n",
    "print(class_weight_dict)\n",
    "\n",
    "for class_name, class_weight in zip(class_names, class_weights):\n",
    "    print(f\"{class_name}: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with adjusting the loss function for 'akiec', 'bcc', and 'mel' to address class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CSV file to store the model results\n",
    "csv_file = \"model_results.csv\"\n",
    "fieldnames = [\n",
    "    \"architecture\",\n",
    "    \"optimizer\",\n",
    "    \"loss\",\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"auc\",\n",
    "]\n",
    "\n",
    "# Write the header to the CSV file. Data is written to the file after model_result\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Define batch size, architecture, and optimizer\n",
    "batch_size = 32\n",
    "architectures = [\"InceptionV3\"]\n",
    "optimizers = [\"Adam\"]\n",
    "\n",
    "# Create directory to store models\n",
    "models_dir = \"models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Initialize model_results to store the evaluation results\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 7 classes.\n",
      "Class Weights: [ 4.36859324  2.78484532  1.30212904 12.44099379  1.28603531  0.21338021\n",
      " 10.04010025]\n",
      "Class Weight Dictionary: {0: 4.368593238822246, 1: 2.7848453249913105, 2: 1.3021290427433772, 3: 12.440993788819876, 4: 1.2860353130016051, 5: 0.21338020666879728, 6: 10.040100250626567}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Loop through architectures and optimizers\n",
    "for architecture in architectures:\n",
    "    for optimizer_name in optimizers:\n",
    "        # Define the optimizer\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "        elif optimizer_name == \"RMSprop\":\n",
    "            optimizer = RMSprop(learning_rate=0.001)\n",
    "        elif optimizer_name == \"SGD\":\n",
    "            optimizer = SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "        # Load and preprocess data using tf.keras.preprocessing\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=preprocessing_function,\n",
    "        )\n",
    "\n",
    "        train_dataset = train_datagen.flow_from_directory(\n",
    "            \"Resources/Skin Cancer/train\",\n",
    "            target_size=input_shape[:2],\n",
    "            batch_size=batch_size,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "        # Calculate class weights\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(train_dataset.classes),\n",
    "            y=train_dataset.classes\n",
    "        )\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        print(\"Class Weights:\", class_weights)\n",
    "        print(\"Class Weight Dictionary:\", class_weight_dict)\n",
    "\n",
    "        # Define the indices of the classes you want to prioritize\n",
    "        priority_classes = ['mel', 'bcc', 'akiec']\n",
    "\n",
    "        # Define the weight multiplier for the priority classes\n",
    "        priority_weight_multiplier = 5  # Adjust this value as needed\n",
    "\n",
    "        # Assign higher weights to the priority classes and keep the weights for other classes unchanged\n",
    "        for class_name, class_weight in zip(class_names, class_weights):\n",
    "            if class_name in priority_classes:\n",
    "                if class_name == 'mel':\n",
    "                    class_weight_dict[class_name] = class_weight * priority_weight_multiplier * 4  # Quadrulpling the multiplier for 'mel'\n",
    "                else:\n",
    "                    class_weight_dict[class_name] = class_weight * priority_weight_multiplier\n",
    "            else:\n",
    "                class_weight_dict[class_name] = class_weight\n",
    "\n",
    "        # Define the loss function with the manually adjusted class weights\n",
    "        def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Custom loss function for weighted categorical crossentropy.\n",
    "\n",
    "            Args:\n",
    "                y_true (tensor): True labels.\n",
    "                y_pred (tensor): Predicted probabilities.\n",
    "\n",
    "            Returns:\n",
    "                tensor: Weighted categorical crossentropy loss.\n",
    "            \"\"\"\n",
    "\n",
    "            # Define a tensor containing the class weights\n",
    "            weights = tf.constant([class_weight_dict[class_name] for class_name in class_names], dtype=tf.float32)\n",
    "\n",
    "            # Clip predicted probabilities to avoid numerical instability\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "            # Compute the categorical cross-entropy loss with weighted class probabilities\n",
    "            loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred) * weights, axis=-1))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # Define the pre-trained model architecture\n",
    "        if architecture == \"VGG16\":\n",
    "            base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "        elif architecture == \"ResNet50\":\n",
    "            base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "        elif architecture == \"InceptionV3\":\n",
    "            base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "\n",
    "        # Freeze the layers of the pre-trained model\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom layers on top of the pre-trained model\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "        # Create the final model\n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        # Compile the model with the weighted loss function\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=weighted_categorical_crossentropy,\n",
    "            metrics=[\"accuracy\", Precision(), Recall(), AUC()],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akiec: 4.368593238822246\n",
      "bcc: 2.7848453249913105\n",
      "bkl: 1.3021290427433772\n",
      "df: 12.440993788819876\n",
      "mel: 1.2860353130016051\n",
      "nv: 0.21338020666879728\n",
      "vasc: 10.040100250626567\n"
     ]
    }
   ],
   "source": [
    "# Print original class weights\n",
    "for class_name, class_weight in zip(class_names, class_weights):\n",
    "    print(f\"{class_name}: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.368593238822246\n",
      "1: 2.7848453249913105\n",
      "2: 1.3021290427433772\n",
      "3: 12.440993788819876\n",
      "4: 1.2860353130016051\n",
      "5: 0.21338020666879728\n",
      "6: 10.040100250626567\n",
      "akiec: 21.84296619411123\n",
      "bcc: 13.924226624956553\n",
      "bkl: 1.3021290427433772\n",
      "df: 12.440993788819876\n",
      "mel: 25.720706260032102\n",
      "nv: 0.21338020666879728\n",
      "vasc: 10.040100250626567\n"
     ]
    }
   ],
   "source": [
    "# Print adjusted class weights\n",
    "for class_name, class_weight in class_weight_dict.items():\n",
    "    print(f\"{class_name}: {class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume from Interrupt \n",
    "Should the overnight run need to be interupted, it is possible to pick up where the interruption left off, and continue training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model architecture and optimizer\n",
    "architecture = \"VGG16\"  # Specify the architecture you want to resume training for\n",
    "optimizer_name = \"Adam\"  # Specify the optimizer you want to resume training with\n",
    "\n",
    "# Load the model \n",
    "model = load_model(os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}.h5\"))\n",
    "\n",
    "# Load the optimizer state\n",
    "with open(os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}_optimizer_state.pkl\"), 'rb') as f:\n",
    "    optimizer_weights = model.load(f)\n",
    "    model.optimizer.set_weights(optimizer_weights)\n",
    "\n",
    "# Resume training from a specific epoch\n",
    "resume_epoch = 5  # Specify the epoch number to resume from\n",
    "for epoch in range(resume_epoch, epochs):\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=train_dataset.samples // 32,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=val_dataset.samples // 32,\n",
    "        epochs=1,\n",
    "        callbacks=[tensorboard_callback, lr_scheduler, checkpoint_callback],\n",
    "        initial_epoch=epoch\n",
    "    )\n",
    "\n",
    "    # Save the optimizer state\n",
    "    with open(os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}_optimizer_state.pkl\"), 'wb') as f:\n",
    "        model.dump(model.optimizer.get_weights(), f)\n",
    "\n",
    "     # Save the optimizer state\n",
    "    with open(\n",
    "                os.path.join(\n",
    "                    models_dir,\n",
    "                    f\"model_{architecture}_{optimizer_name}_optimizer_state.pkl\",\n",
    "                ),\n",
    "                \"wb\",\n",
    "            ) as f:\n",
    "                model.dump(model.optimizer.get_weights(), f)\n",
    "\n",
    "        # Evaluate the model on the validation set\n",
    "        loss, accuracy, precision, recall, auc = model.evaluate(val_dataset)\n",
    "        print(f\"Model: {architecture}, Optimizer: {optimizer_name}\")\n",
    "        print(f\"Validation Loss: {loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Validation Precision: {precision:.4f}\")\n",
    "        print(f\"Validation Recall: {recall:.4f}\")\n",
    "        print(f\"Validation AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "        # Append the model results to the list\n",
    "        model_results.append(\n",
    "            {\n",
    "                \"architecture\": architecture,\n",
    "                \"optimizer\": optimizer_name,\n",
    "                \"loss\": loss,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"auc\": auc,\n",
    "            }\n",
    "        )\n",
    "        # Save the model\n",
    "        model.save(\n",
    "            os.path.join(models_dir, f\"model_{architecture}_{optimizer_name}.h5\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Visualizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2003 images belonging to 7 classes.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 82\u001b[0m\n\u001b[0;32m     77\u001b[0m class_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(val_dataset\u001b[38;5;241m.\u001b[39mclass_indices\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m optimizer_name \u001b[38;5;129;01min\u001b[39;00m optimizers:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/models/model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted_categorical_crossentropy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_categorical_crossentropy\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# Make predictions on the validation dataset using the redefined val_dataset\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(val_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:145\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    143\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _deserialize_nested_config(losses\u001b[38;5;241m.\u001b[39mdeserialize, loss_config)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_compile_arguments_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Recover metrics.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrew Koller\\anaconda3\\envs\\tf_new\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:245\u001b[0m, in \u001b[0;36m_resolve_compile_arguments_compat\u001b[1;34m(obj, obj_config, module)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resolves backwards compatibility issues with training config arguments.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03mThis helper function accepts built-in Keras modules such as optimizers,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03mthis does nothing.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m obj \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mALL_OBJECTS_DICT:\n\u001b[1;32m--> 245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mget(\u001b[43mobj_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Importing Dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_results = \"model_results.csv\"\n",
    "\n",
    "# Run Number - use to create new directory or add to existing directory\n",
    "run_number = 6\n",
    "run_dir = f\"run{run_number}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "architectures = [\"InceptionV3\"] \n",
    "optimizers = [\"Adam\"]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Define the preprocessing function based on the architectures you want to visualize\n",
    "architectures_to_visualize = [\"InceptionV3\"]  # Specify the architectures you want to visualize\n",
    "preprocessing_functions = {\n",
    "    \"VGG16\": tf.keras.applications.vgg16.preprocess_input,\n",
    "    \"ResNet50\": tf.keras.applications.resnet50.preprocess_input,\n",
    "    \"InceptionV3\": tf.keras.applications.inception_v3.preprocess_input\n",
    "}\n",
    "\n",
    "\n",
    "# Create a directory to store the visualization results\n",
    "visualizations_dir = f\"{run_dir}/visualizations\"\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "#\n",
    "\n",
    "# Iterate over each model architecture and optimizer\n",
    "for architecture in architectures:\n",
    "    # Set the preprocessing function based on the architecture\n",
    "    preprocessing_function = preprocessing_functions[architecture]\n",
    "\n",
    "    # Set the input shape and preprocessing function based on the selected architecture\n",
    "    if architecture == \"VGG16\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "    elif architecture == \"ResNet50\":\n",
    "        input_shape = (224, 224, 3)\n",
    "        preprocessing_function = tf.keras.applications.resnet50.preprocess_input\n",
    "    elif architecture == \"InceptionV3\":\n",
    "        input_shape = (299, 299, 3)\n",
    "        preprocessing_function = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    # Redefine the validation dataset with the corresponding preprocessing function\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "    val_dataset = val_datagen.flow_from_directory(\n",
    "        \"Resources/Skin Cancer/val\",\n",
    "        target_size=input_shape[:2],\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "    )\n",
    "\n",
    "    # Get the class names from the redefined val_dataset\n",
    "    class_names = list(val_dataset.class_indices.keys())\n",
    "\n",
    "\n",
    "    for optimizer_name in optimizers:\n",
    "        # Load the trained model\n",
    "        model = load_model(os.path.join(f\"{run_dir}/models/model_{architecture}_{optimizer_name}.h5\"), custom_objects={'weighted_categorical_crossentropy': weighted_categorical_crossentropy})\n",
    "\n",
    "        # Make predictions on the validation dataset using the redefined val_dataset\n",
    "        y_pred = model.predict(val_dataset)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Get the true labels of the validation dataset\n",
    "        y_true = val_dataset.classes\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred_classes)\n",
    "        print(f\"Confusion Matrix for {architecture}_{optimizer_name}:\")\n",
    "        print(cm)\n",
    "\n",
    "        # Save the confusion matrix as a CSV file\n",
    "        cm_filename = f\"confusion_matrix_{architecture}_{optimizer_name}.csv\"\n",
    "        np.savetxt(os.path.join(visualizations_dir, cm_filename), cm, delimiter=\",\")\n",
    "\n",
    "        # Compute the classification report\n",
    "        cr = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
    "        print(f\"Classification Report for {architecture}_{optimizer_name}:\")\n",
    "        print(cr)\n",
    "\n",
    "        # Save the classification report as a text file\n",
    "        cr_filename = f\"classification_report_{architecture}_{optimizer_name}.txt\"\n",
    "        with open(os.path.join(visualizations_dir, cr_filename), \"w\") as file:\n",
    "            file.write(cr)\n",
    "\n",
    "        # Compute the ROC curve and AUC for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(len(class_names)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Plot the ROC curve for each class\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i in range(len(class_names)):\n",
    "            plt.plot(\n",
    "                fpr[i],\n",
    "                tpr[i],\n",
    "                label=f\"ROC curve of class {class_names[i]} (AUC = {roc_auc[i]:.2f})\",\n",
    "            )\n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve for {architecture}_{optimizer_name}\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(visualizations_dir, f\"roc_curve_{architecture}_{optimizer_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Visualize the model's predictions on a subset of the validation data\n",
    "        subset_size = 10\n",
    "        subset_indices = np.random.choice(len(val_dataset), subset_size, replace=False)\n",
    "        subset_images = []\n",
    "        subset_labels = []\n",
    "        val_dataset.reset()  # Reset the validation dataset iterator\n",
    "        for i in range(len(val_dataset)):\n",
    "            if i in subset_indices:\n",
    "                image_batch, label_batch = next(val_dataset)\n",
    "                for image, label in zip(image_batch, label_batch):\n",
    "                    subset_images.append(image)\n",
    "                    subset_labels.append(label)\n",
    "\n",
    "        subset_images = np.array(subset_images)\n",
    "        subset_labels = np.array(subset_labels)\n",
    "\n",
    "        subset_preds = model.predict(subset_images)\n",
    "        subset_pred_classes = np.argmax(subset_preds, axis=1)\n",
    "\n",
    "        # Generate the visualization plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(subset_size):\n",
    "            plt.subplot(2, 5, i + 1)\n",
    "            plt.imshow(subset_images[i])\n",
    "            plt.title(\n",
    "                f\"True: {class_names[np.argmax(subset_labels[i])]}\\\\nPred: {class_names[subset_pred_classes[i]]}\"\n",
    "            )\n",
    "            plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(visualizations_dir, f\"predictions_{architecture}_{optimizer_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Read the CSV file and store the model results\n",
    "model_results = []\n",
    "with open(\"model_results.csv\", \"r\") as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        model_results.append(row)\n",
    "\n",
    "# Create a bar plot for validation precision\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"precision\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation Precision\")\n",
    "plt.title(\"Validation Precision for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_precision_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot for validation recall\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"recall\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation Recall\")\n",
    "plt.title(\"Validation Recall for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_recall_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot for validation AUC-ROC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"auc\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation AUC-ROC\")\n",
    "plt.title(\"Validation AUC-ROC for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_auc_comparison.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Visualizations Outside of Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "visualizations_dir = \"visualizations\"\n",
    "\n",
    "## Read the CSV file and store the model results\n",
    "model_results = []\n",
    "with open(\"model_results.csv\", \"r\") as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        model_results.append(row)\n",
    "\n",
    "# Create a bar plot for validation precision\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"precision\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation Precision\")\n",
    "plt.title(\"Validation Precision for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_precision_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot for validation recall\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"recall\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation Recall\")\n",
    "plt.title(\"Validation Recall for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_recall_comparison.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create a bar plot for validation AUC-ROC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(model_results)), [float(result[\"auc\"]) for result in model_results])\n",
    "plt.xticks(\n",
    "    range(len(model_results)),\n",
    "    [f\"{result['architecture']}_{result['optimizer']}\" for result in model_results],\n",
    "    rotation=45,\n",
    ")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Validation AUC-ROC\")\n",
    "plt.title(\"Validation AUC-ROC for Different Models\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, \"validation_auc_comparison.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
